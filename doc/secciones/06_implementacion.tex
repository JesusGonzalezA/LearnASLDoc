\chapter{Implementation}

\section{Backend code documentation}
    \subsection{Architecture}
        \subsubsection{From MVC to Onion Architecture}
            \textbf{Model View Controller} (MVC) is the most commonly used web application architecture. 
            It solves the separation of concern as there is a separation between \textit{View} (in this particular case the HTTP API responses), 
            the \textit{Controller} (on which the business logic is added) and the \textit{Model} (it includes the database access).

            As the outer layers of the application always communicate with the inner layers via interfaces, it develops a loosely coupled application \cite{benyusouf}.

            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.5\textwidth]{assets/mvc.png}
                \caption{MVC architecture \cite{MVC}}
                \label{fig:implementation_mvc}
            \end{figure}

            The \textbf{Onion Architecture} is very similar to \textit{MVC} because it keeps the separation of concerns but it solves the issue of tight coupling.

            \paragraph \textbf{How Onion architecture solves tight coupling?} \\
                The onion architecture is mainly based on Dependency Inversion Principle. Each concern refers to a layer, so the application is 
                divided in multiple layers that communicate with each other using interfaces.
                This is also very convenient for testing purposes, due to the ease of exchanging a layer for a mock or another implementation object. \\

                The layers can change, but the idea is that all layers are towards to center, where the Core of the application is. It represents the business and behavior objects. \\

                \begin{figure}[H]
                    \centering
                        \includegraphics[width=\textwidth]{assets/onion.png}
                    \caption{Onion architecture \cite{OnionArchitecture}}
                    \label{fig:implementation_onion}
                \end{figure}

        \subsubsection{How to orgnanize the project}
            The project is divided in multiple layers. Each layer will represent a solution, as it will have its own dependencies, references 
            and could be independent from the implementation of other layers by just fulfilling the contracts defined by the interfaces.
            \begin{itemize}[noitemsep]
                \item Test: it is used for testing purposes.
                    \begin{itemize}[noitemsep]
                        \item References: all other layers.
                        \item Dependencies: xUnit.
                    \end{itemize}
                \item API: it is similar to the view layer in MVC. It is the layer that will allow a client to interact with the application.
                    \begin{itemize}[noitemsep]
                    \item Reponsabilities of every directory:
                            \begin{itemize}[noitemsep]
                                \item Controllers: defines all the endpoints of the application and call the correct services on each method.
                                \item Middleware: catches users' calls to the API. It allows to have a global exception handler, authorization, authentication...
                                \item Properties: defines the launch settings for the Http Server.
                            \end{itemize}
                        \item References: Core
                        \item Dependencies:
                            \begin{itemize}[noitemsep]
                                \item API Rest: Asp.Net Core 5.0
                                \item Documentation: Swagger
                            \end{itemize}
                    \end{itemize}
                \item Core: it is the center part of the architecture. 
                It contains the definition of all the entities of the application (contracts for the REST API, auxiliary entities and data access entities) and the 
                interfaces to allow communication between layers. 
                The Core also contains all the business logic and works with the data source.
                    \begin{itemize}[noitemsep] 
                        \item Responsabilities of every namespace:
                            \begin{itemize}[noitemsep]
                                \item Contracts: instead of sending and receiving the entities in the API, a good approach is to use DTO (Data Transfer Objects). This way, the user don't need to send the Id before creating an entity, we can avoid to send properties that may be confidential such as passwords and we can retrieve just the needed information. All these DTOs are defined in this directory.
                                \item CustomEntities: definition of auxiliary entities.
                                \item Entities: definition of data access models.
                                \item Enums
                                \item Exceptions: definition of custom exceptions.
                                \item Extensions: helper functions to improve code readability.
                                \item Interfaces: interfaces for repositories and services of the app.
                                \item Options: implementation of options pattern to get the configuration variables from an extern source rather than hard-coded values.
                                \item QueryFilters: filters for get API methods.
                                \item Services: implementation of services to control the data access.
                                \item ValidationAttributes: attributes to validate the models.
                            \end{itemize}
                        \item References: None
                        \item Dependencies: None
                    \end{itemize}
                \item Infraestructure: it contains the concrete implementations of all the services needed by the app to work.
                    \begin{itemize}[noitemsep]
                        \item Responsabilities of every namespace:
                            \begin{itemize}[noitemsep]
                                \item Data: definition of the database context and migrations.
                                \item Extensions: helper functions to improve code readability.
                                \item Factories: implements abstract factory pattern to initialize the questions.
                                \item Filters: adds the correct flows to validate the model in the API.
                                \item Mappings: to convert an entity to a contract and reverse.
                                \item Migrations: database creation and updates.
                                \item Repositories: implementation of repositories and unit of work pattern.
                                \item Services: implementation of services based on external implementations. For example: service to hash a password, send an email, create a JWT...
                                \item Validators: validation rules for the API filters to validate the model. For example: email should be valid, password should have 'x' length, date should be greater than 'x'...
                            \end{itemize}
                        \item References: Core
                        \item Dependencies:
                            \begin{itemize}[noitemsep]
                                \item Mail client: MailKit
                                \item ORM: Entity Framework Core
                                \item Validations: Fluent Validation
                                \item Logging: Serilog
                                \item Authentication: JWT
                                \item Mappings: automapper
                            \end{itemize}
                    \end{itemize}
            \end{itemize}

        \subsubsection{Dependency injection}
            As this is a big project, following good design principles and patterns is important for its correct development and later maintenance.

            As the D in the \textbf{SOLID principles} dictate (Dependency Inversion Principle), avoid depending on concrete implementations is a good practice as the application and its services may change. \\
            Instead of having high-level layers depending on low-level layers' implementations, they depend on their abstractions. 
            This way, the code in the upper layers won't change if the concrete implementation of a low-level layer does, as the method signatures won't change.

            In ASP.NET it is recommended to use a dependency injection container and inject the dependencies in the constructor \cite{DI}. 
            In other words, I only use interfaces in my code and I do not instanciate any object. The framework does this job for me, but I first need to 
            teach the framework how to change from an interface to a concrete implementation (instanciate an object that satisfies that interface and cast it).
            The next fragment of code \ref{lst:impl_di_config} shows how to set this up. The methods \textit{AddTransient<Interface, Implementation>()}, \textit{AddSingleton<Interface, Implementation>()}, \textit{AddScoped<Interface, Implementation>()} define 
            which implementation to use whenever the framework sees an interface. The difference between them is that they define a different lifetime to the initialized object. 
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Dependency injection configuration}, 
                label={lst:impl_di_config}]
            {code/DependencyInjectionConfig.cs}
            
            As I said before, the dependencies are injected in the constructor. This is, if an object depends on a low-level layer, it will have 
            an attribute that fulfills the interface needed. The framework will automatically initialize the attribute passing the correct 
            implementation declared in the dependency injection container.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Dependency injection usage}, 
                label={lst:impl_di_usage}]
            {code/DependencyInjectionUsage.cs}

            \paragraph{Lifetime of an object}
                \begin{itemize}[noitemsep]
                    \item \textbf{Transient:} services are created each time they're requested. Best for lightweight, stateless services.
                    \item \textbf{Scoped:} services are created once per client request (connection). Better option when you want to maintain state within a request.
                    \item \textbf{Singleton:} services are created the first time they're requested.
                \end{itemize}
    
    \subsection{UML Class Diagrams}
        Due to the size of the application, documenting all the components of the application in a single class diagram would be 
        unintelligible. For this reason, I have divided the diagram into two diagrams: 'the entities, contracts and enums' and the 'interfaces' one.
        The \textbf{entities, contracts and enums class diagram} presents the relationships and definition of all the objects that will be stored in the database, 
        the contracts with external services and enums.
        The \textbf{interfaces class diagram} presents all the interfaces that both repositories and services should implement. 
    
        \subsubsection{Class diagram: entities, contracts and enums}
            \textit{To see it in an optimized view, please click \href{https://github.com/JesusGonzalezA/LearnASLDoc/blob/master/doc/assets/diagrams/interfaces.png}{here}.} \\
            Figure \ref{fig:implementation_enums} defines all the enums used in the application (difficulty and the test types).
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.7\textwidth]{assets/diagrams/enums.png}
                \caption{Class diagram: enums}
                \label{fig:implementation_enums}
            \end{figure}

            Figure \ref{fig:implementation_exceptions_clasess} presents the exceptions thrown by me in the application and their relationship with 
            the exception class implemented in the language.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.7\textwidth]{assets/diagrams/exceptions_classes.png}
                \caption{Class diagram: classes to allow capturing custom exceptions}
                \label{fig:implementation_exceptions_clasess}
            \end{figure}

            Figure \ref{fig:implementation_auxiliary} shows auxiliary classes. \textbf{Metadata} is a generic class that allow me to send information about a paginated 
            list through the API. The \textbf{PagedList} is an implementation that allow me to send paginated lists to the client. This way I can divide the information into chunks
            for a better performance. The \textbf{UpdateQuestionParameters} class is only used to refactor some functions.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/auxiliary.png}
                \caption{Class diagram: auxiliary classes to make some arguments cleaner and allow sending paginated objects and metadata to the client}
                \label{fig:implementation_auxiliary}
            \end{figure}

            The next diagram (see figure \ref{fig:implementation_error_response}) represents a class used to send errors from the API in a consistent way. 
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.5\textwidth]{assets/diagrams/error_response.png}
                \caption{Class diagram: class to send errors from the API in a consistent way}
                \label{fig:implementation_error_response}
            \end{figure}

            The figure \ref{fig:implementation_entities_db} presents the main entities of the application, those that 
            are going to be really stored in the database. The only relationship shown is the inherit relationship, as the diagram would be intelligible. To better understand the 
            relationships between the entities see the optimized view. \\
            From left to right, \textbf{BaseQuestionEntity} (and children) define the neccessary classes to store 
            information about each question that will form the tests, \textbf{DatasetItemEntity} defines an item of the dataset and it's used to create the questions, 
            \textbf{ErrorWordEntity and LearntWordEntity} are used to store the words from the mistaken questions by the users and for statistic purposes, \textbf{UserEntity}
            is a class used to store the information about an user, \textbf{TestEntity} is used to store the information about each test.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/entities_bd.png}
                \caption{Class diagram: objects stored in the database}
                \label{fig:implementation_entities_db}
            \end{figure}

            The diagram from figure \ref{fig:implementation_dto_1} presents all the entities that the API is going to receive from the users. This is created to 
            better separate the responsabilities from a data/model entity and a presentation entity, to validate the objects before working with them and for a better
            maintenance of the project.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/incoming_dto.png}
                \caption{Class diagram: incoming DTOs}
                \label{fig:implementation_dto_1}
            \end{figure}

            As you may notice, figure \ref{fig:implementation_entities_db} presents the DTOs that are going to be sent from the API to the client. The objects would be
            the same as the objects stored in the database, but they exclude sensible properties such as passwords and may have all the information from their relationships
            with other objects (populated).
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/outcoming_dto.png}
                \caption{Class diagram: objects stored in the database}
                \label{fig:implementation_entities_db}
            \end{figure}
        
            
        \newpage
        \subsection{Class diagram: interfaces}
            \textit{To see it in an optimized view, please click \href{https://github.com/JesusGonzalezA/LearnASLDoc/blob/master/doc/assets/diagrams/interfaces.png}{here}.} \\
            The next figure \ref{fig:implementation_interfaces} contains the definition of the interfaces used by the services from the core of the application. Their implementations 
            would include the business logic of the application.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/interfaces.png}
                \caption{Class diagram: interfaces for core services}
                \label{fig:implementation_interfaces}
            \end{figure}

            \newpage
            The next figure \ref{fig:implementation_interfaces_2} contains the definition of the interfaces used by the services from the infraestructure of the application. Their implementations 
            would contain concrete implementation based on extern services, such as email service, storage service, hashing service, etc.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/interfaces_2.png}
                \caption{Class diagram: interfaces for infraestructure services}
                \label{fig:implementation_interfaces_2}
            \end{figure}

        \newpage
        \subsection{Abstract factory to generate the questions}
            All the logic to create the questions should be changed if a new type of test is added. 
            To avoid this, due to maintenance reasons I opted to use this pattern to generate the questions. 

            \begin{figure}[H]
                \centering
                    \includegraphics[width=\textwidth]{assets/diagrams/abstractfactory1.png}
                \caption{Abstract factory to generate the questions (compact version)}
                \label{fig:implementation_af1}
            \end{figure}
            \newpage
            \begin{figure}[H]
                \centering
                    \includegraphics[angle=90, height=\textheight]{assets/diagrams/abstractfactory.png}
                \caption{Abstract factory to generate the questions (extended version)}
                \label{fig:implementation_af}
            \end{figure}

    \subsection{Defining environment variables and storing secrets}
        As the application grows, it is important to use environment variables to configure its behaviour without the neccesity to compile it again, to 
        divide the application into environments (development, test, production) and to avoid using hard-coded values for a better maintenability and 
        semantic.

        \subsubsection{Options pattern}
            The values can be defined in a JSON, plain text files, cloud or using other tools.
            In my case, the settings variables are defined in a JSON and using a tool from Microsoft.
            In general, the proccess would be: defining the values, reading the values and storing them in 
            objects that group the ones that are related.
            This pattern allow me to do this.

            After defining the values, the first thing to do is defining these strongly-typed objects that will store the values. 
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Options: defining the objects}, 
                label={lst:impl_options}]
            {code/Options_1.cs}
            
            Then, I define how to map from those configuration files to these objects.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Options: defining the mapping}, 
                label={lst:impl_options2}]
            {code/Options_2.cs}

            The framework use behind dependency injection. This is, I can inject the interface and the framework will instanciate and fill the object with 
            its correct values from the setting files as it knows how to do the mapping.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Options: using the initialized objects}, 
                label={lst:impl_options3}]
            {code/Options_2.cs}

        \subsubsection{Microsoft's development implementation: User secrets}
            \textit{See \href{https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-5.0&tabs=windows}{here} for more information.} 
            Every piece of sensitive data is considered a secret. It is not safe to define the secrets of the application using plain-text files, 
            or using the configuration JSON files. It is a better option to use the \textit{Secret Manager Tool} provided by \textit{Microsoft}.
            Nevertheless, it is not safe in a production environment as it is not encrypted. Just for testing and local development 
            purposes you can configure it this way:

            Firstly, defining all your secrets on a JSON file.
            \lstinputlisting[escapechar=+, language=json, captionpos=t,
                caption={JSON containing the definition for the user secrets}, 
                label={lst:impl_user_secrets_input}]
            {code/secrets.json}

            Finally, initializing the tool and saving the JSON file on it.
            \lstinputlisting[escapechar=+, language=bash, captionpos=t,
                caption={Init and configure user secrets}, 
                label={lst:impl_user_secrets}]
            {code/usersecrets.bash}

        \subsubsection{Moving to a production environment}
            Microsoft recommends to use \textbf{Azure Key Vault} \cite{AKV} to store the secrets of the application on its cloud, as they will be 
            stored in an encrypted file and you can set the permissions for your development team.

    \subsection{Using extension methods to clean startup method}
        As the services and the configuration grow in the application, the \textit{Startup} class becomes less maintenable. Therefore,
        a good approach is to use extension methods to save the logic of every configuration method in a different file.
        
        The next fragment of code \ref{lst:impl_startup_refactor} shows the new \textit{Startup} class. Now the reason of change of this file would be 
        adding or deleting some type of configuration, not the concrete implementation of how to configure the application.
        \lstinputlisting[escapechar=+, language=CSharp, captionpos=t,
            caption={Refactoring startup method}, 
            label={lst:impl_startup_refactor}]
        {code/StartupExtensions.cs}

        This is achieved using extension methods as shown in the code \ref{lst:impl_startup_extensions_1}. Using the \textit{this} keyword as argument of a static method,
        we can add the method to the type that follows the \textit{this} keyword.
        \lstinputlisting[language=CSharp, captionpos=t,
            caption={Service collection's extensions methods}, 
            label={lst:impl_startup_extensions_1}]
        {code/StartupExtensions1.cs}

    \subsection{Entity framework}
        \subsubsection{Code first}
            Following \textit{Donatas Kimutis}'s recommendations \cite{Kimutis}, I use this approach. Firstly, I define the classes and mappings in the code. Then, the databases are created from the code. All evolutions are made via migrations.
            To create a migration and then apply the changes to the database, use the next commands \ref{lst:impl_migration}.
            \lstinputlisting[escapechar=+, language=bash, captionpos=t,breaklines=true,
                caption={Migrating the database}, 
                label={lst:impl_migration}]
            {code/migration.bash}

            In the next fragment of code \ref{lst:impl_userentity} I present the \textbf{User entity}, which has the following properties:
            \begin{itemize}[noitemsep]
                \item Email
                \item Password
                \item ConfirmedEmail
                \item TokenPasswordRecovery (optional)
                \item TokenEmailConfirmation (optional)
                \item Tests (navigation property). A user can have 0 or multiple tests.
            \end{itemize}
            The \textit{virtual} keyword is used to mark navigational properties. This also enables \textit{Lazy Loading}, to allow to load the properties only when they're used to improve the performance.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={User entity model creation}, 
                label={lst:impl_userentity}]
            {code/Virtual.cs}

        \subsubsection{Entity}
            All objects that will be stored in the database will inherit from the \textbf{BaseEntity} class. 
            This class will be \textit{abstract}, as won't be instanciated and will allow me to create generic repositories and 
            track events to those objects, such as adding or updating tuples from the database. This will be used to create stats.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Entity class}, 
                label={lst:impl_entity}]
            {code/Entity.cs}

        \subsubsection{Automatically tracking entities through events}
            \textit{For this section I followed this \href{https://docs.microsoft.com/en-us/dotnet/API/microsoft.entityframeworkcore.changetracking.changetracker?view=efcore-5.0}{guide}.}\\
            When an operation with the database is made, the \textit{Entity framework} tracks the entity.  
            In the constructor of the \textit{DatabaseContext} class, I set up two methods that will be executed when two events are fired:
            \begin{itemize}[noitemsep]
                \item When the event \textbf{StateChanged} is fired, the method \textit{OnEntityStateChanged} will be executed. It will update the parameter \textit{ModifiedOn} from the \textit{Entity} if the tracked entity is being modified.
                \item When the event \textbf{Tracked} is fired, the method \textit{OnEntityTracked} will be executed. It will update the parameter \textit{CreatedOn} from the \textit{Entity} if the tracked entity has been created.
            \end{itemize}

            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Entity tracking}, 
                label={lst:impl_entity_tracking}]
            {code/EntityTracking.cs}
        
        \subsubsection{Repository pattern}
            \textit{For this section, I used a guide from Microsoft \cite{RepoAndUW}.}\\
            It is intended to add an abstraction layer between the data access and the business logic layer. 
            To implement the repository pattern, I need to create an interface and an implementation for every entity in the application.
            As some functions are common for these entities, I can create a generic repository.

            In the next code fragment \ref{lst:impl_ibaserepository}, I define the generic interface for the repositories. 
            Note that the generic type \textit{T} should inherit from the \textit{BaseEntity}, so I only use this pattern in objects that will
            be stored in the database.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Interface for the generic repository}, 
                label={lst:impl_ibaserepository}]
            {code/IBaseRepository.cs}

        \subsubsection{Unit of work pattern}
            {For this section, I used a guide from Microsoft \cite{RepoAndUW}.}\\
            As the application grows, more entities and repositories appear. The \textit{unit of work pattern} 
            makes the use of these repositories cleaner and it ensures that all repositories share a single database context.

            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/diagrams/unitofwork.png}
                \caption{Unit of Work}
                \label{fig:implementation_unit_work}
            \end{figure}
    
    \subsection{Error handling middleware}
        It is important to handle the errors of the application. Good reasons to do it could be to find errors in the code, 
        send alerts if something has failed or stopped working, to avoid sending sensitive information to the client, etc.

        As we can see in the next code fragment \ref{lst:impl_ehm}, in the \textit{Startup} class I define the use of this middleware.
        The middleware capture the exceptions from the rest of middlewares and execution from the controllers. \\
        \lstinputlisting[language=CSharp, captionpos=t,
            caption={Error handling middleware: setup}, 
            label={lst:impl_ehm}]
        {code/ErrorHandlingMiddleware.cs}

        In the next code fragment \ref{lst:impl_ehm1}, the middleware is defined. It catches all exceptions, but there is a consideration when 
        they are custom exceptions (\textit{BusinessExceptions} and \textit{ControllerExceptions}).
        The custom exceptions are thrown by me, so I just send the error message to the client with a custom code.
        On the other hand, the rest of the exceptions are not handled by me. 
        These could be network exceptions, from other dependencies such as the \textit{ORM}, etc. These exceptions are logged and
        the stack trace is sent to the client if the environment is set to \textit{Development}, to avoid returning more information than neccessary.
        \lstinputlisting[language=CSharp, captionpos=t,
            caption={Error handling middleware}, 
            label={lst:impl_ehm1}]
        {code/ErrorHandlingMiddleware.cs}

        \subsubsection{Logger}
            All logs are saved in the \textit{Log} directory. The name of the file comes from the day. Every log entry contains the next information:
            \begin{itemize}[noitemsep]
                \item Date and hour
                \item HTTP method from the request
                \item Route of the request
                \item Stack trace
            \end{itemize}
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.3\textwidth]{assets/logs.png}
                \caption{Logs}
                \label{fig:implementation_logs}
            \end{figure}
        
    \subsection{Mapping}
        As Microsoft states \cite{DTO}, there are some cases in which you don't want to expose your database entities to the client in an \textit{API}. These cases could be:
        \begin{itemize}[noitemsep]
            \item Remove circular references
            \item Hide properties 
            \item Reduce payload size
            \item Populate nested objects 
            \item Protect sensitive data
            \item Decouple the service layer from the database layer
        \end{itemize}

        In conclusion, the data transfer object is an entity with all the data you need to send to the client. It is constructed from the database entity. 
        Therefore, you should have methods to transform a \textit{DTO} to a \textit{Database object} and viceversa. This process is called \textbf{Mapping}.

        \subsubsection{AutoMapper}
            To make the mapping process easier, I used a library called \textit{AutoMapper} \cite{AutoMapper}.

            In the next code fragment \ref{lst:impl_automapper}, I show how to set \textit{AutoMapper} up. 
            The first thing to do is creating a profile. In this case, I show a profile to map from \textit{UserDto} to \textit{UserEntity}, which is the database entity. 
            Note that this map is reversible and, as both entities inherits from a base class, this class should have an own mapping too. 
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Mapping example: creating the profile}, 
                label={lst:impl_automapper}]
            {code/AutoMapper.cs}
            
            The next thing to do is creating adding the profile to the \textit{mapper configuration} and setting up in the \textit{services} (see code \ref{lst:impl_automapper1}).
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Mapping example: configuration}, 
                label={lst:impl_automapper1}]
            {code/AutoMapper_1.cs}

            Then, I can simply use it to map from an entity to the other (see code \ref{lst:impl_automapper2}). 
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Mapping example: use}, 
                label={lst:impl_automapper2}]
            {code/AutoMapper.cs}

    \subsection{Filters: automatic validation}
        According to Microsoft \cite{Filters}, filters allow to run code before or after specific stages in the request process pipeline. 
        For example, I use filters that run before the controller action and aim to avoid running unnecessary code due to the fact that the 
        request is badly formed.

        \subsubsection{Using validators}
            I use a library called \textit{FluentValidation} \cite{FluentValidation}. This one allows a strongly-typed validation.

            In the next fragment code \ref{lst:impl_filtervalidator}, I show how to create a new custom validator and set it up in the \textit{Startup} class.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Setting up the pipeline for a filter to validate the Login DTO}, 
                label={lst:impl_filtervalidator}]
            {code/Filters_Validator_Setup.cs}

            In the next fragment of code \ref{lst:impl_filtervalidator1}, I show the class \textit{LoginDtoValidator}, which is a validator for the \textit{DTO} object \textit{LoginDto}. It adds rules for the object to not have an empty \textit{Email}, not having an empty \textit{Password} and its length from 6 to 15 and having a password that matches a \textit{RegEx} that checks if it has at least a lowercase and uppercase letter, if it has a digit and if it has at least a symbol.
            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Implementing a filter to validate the Login DTO}, 
                label={lst:impl_filtervalidator1}]
            {code/Filters_Validator.cs}

        \subsubsection{Using attributes}
            They are called \textbf{Data annotations}. They serve the same purpose (adding a new stage to the request pipeline to validate the arguments sent by the user), 
            but it is less powerful, as they do not support conditional validations, they do not support validations in sets 
            (check the field 'x' when the field 'y' is 'z'...).

            For simple cases, such as the next one, I only use this type of validation. 
            In the next code fragment \ref{lst:impl_filterattributes} I show how to create an attribute to check if the model is valid or not. 
            It checks the length of a file. If it is bigger than the configured value, the model is incorrect.

            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Configure an attribute to validate the max file of a file}, 
                label={lst:impl_filterattributes}]
            {code/Filters_attributes.cs}

    \subsection{Security}
        \subsubsection{Hashing passwords}
            As Microsoft recommends \cite{Hash}, I use \textbf{Pbkdf2 algorithm} to hash the passwords. It is a key derivation function to reduce brute-force attacks.

            The algorithm receives as arguments the \textit{password, salt and iterations}. It is important to think in the check process before hashing the password.
            In order to check the encrypted password, I would need the same arguments. If the salt or iterations varied from a password to another one, I should include this into the tuple in the database too. 
            
            This way, every password could have a different salt and/or iterations. I would read the encrypted password, the salt and the iterations that I used to hash the password and I could check it this way.
            A way of doing it could be having this structure: \textit{<iterations>.<salt>.<encrypted\_key>}.
            
            The next fragment of code \ref{lst:impl_hash} shows how to implement this.

            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Method to hash a password}, 
                label={lst:impl_hash}]
            {code/PasswordService_Hash.cs}

            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/users_database.png}
                \caption{Encrypted passwords in the database}
                \label{fig:user_database}
            \end{figure}

            As I said before, the structure of the tuple in the database was \textit{<iterations>.<salt>.<encrypted\_key>}. This way, I could split the password from the database into three parts by the character '.'. \\

            The first part should be the number of iterations, the second one should be the salt and the third part the encrypted key. These would be the arguments to check the password. \\

            In the next fragment of code \ref{lst:impl_password} I show how to implement this.

            \lstinputlisting[language=CSharp, captionpos=t,
                caption={Method to check a password}, 
                label={lst:impl_password}]
            {code/PasswordService_Check.cs}

        \subsubsection{JWT tokens}
            A JWT (Json Web Token) \cite{JWT.io} is a standard way to share claims securely between two parties. The token is divided in three parts. Each part is divided by the character '.'.
            \begin{itemize}[noitemsep]
                \item The first part includes information about the algorithm and token type.
                \item The second part is the payload. It includes all the claims. As it could be read, it should not have confidential information.
                \item The third part is the verify signature.
            \end{itemize}

            As the tokens are signed, the API only accepts tokens created by this own application (authorization). Also, as it can store information, such as 
            an identifier of the user, it avoid using sessions because every request has the neccessary information for the API to authenticate the user.
            Also, the tokens have a issuing date. This way, you can check the date and only validate those signed before 'x' date.

            In order to communicate with the server, you need to include the \textit{Authorization} header in the HTTP request. The structure would be: \textit{Authorization: Bearer <your\_jwt>}.

            In the next figure \ref{fig:user_unauthorized}, I show how to send a request to a protected route in the API. The response is unauthorized, as the token is invalid.
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/unauthorized.png}
                \caption{Unauthorized response}
                \label{fig:user_unauthorized}
            \end{figure}

            The token was invalid, so the user would have to login in the app to get access to a token (see figure \ref{fig:user_login_api}).
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/login.png}
                \caption{Login in the API}
                \label{fig:user_login_api}
            \end{figure}

            In the next figure \ref{fig:user_jwtclaims}, you can see the payload from the token. It has the following claims:
            \begin{itemize}[noitemsep]
                \item \textbf{nbf:} not valid before (seconds since Unix epoch).
                \item \textbf{exp:} expiration time (seconds since Unix epoch).
                \item \textbf{emailaddress:} email address of the user.
                \item \textbf{nameidentifier:} unique identifier.
            \end{itemize}
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/jwt.io.png}
                \caption{JWT claims}
                \label{fig:user_jwtclaims}
            \end{figure}

            Now the user can interact with the API as they have a correct token (see figure \ref{fig:user_authorized}).
            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/authorized.png}
                \caption{Authorized response}
                \label{fig:user_authorized}
            \end{figure}

        \subsubsection{Configuring CORS}
            As MDN indicates \cite{CORS}, CORS (Cross-Origin Resource Sharing) is an HTTP header mechanism that allow a server to indicate other than its own from which a browser should permit loading resources.
            Browsers do a \textit{preflight} request before doing the actual one to check if the server allows the request from this origin and this concrete method.

            In the next fragment of code \ref{lst:impl_cors}, I show how to implement the CORS policy in the app. It allows any method from the frontend origin.
            \lstinputlisting[language=CSharp, captionpos=t,
                    caption={Configuring CORS}, 
                    label={lst:impl_cors}]
            {code/Cors.cs}

    \subsection{Email service}
        In order to register in the application, a requisite is to have a valid email account. This is because the account will be checked,
        so only real users with their real accounts will be registered.

        Also, this allows to recover a password easily from the email and, in future versions, create email lists.

        \subsubsection{Authorization flux}
            The process could have been done using \textit{SSO}, \textit{OAuth flux}...
            Nevertheless, I was recommended by my teachers in \textit{Vilnius University} to use my own authorization flux in order to learn.

            \paragraph{Registration flux}
                This way, the flux of registration would be the following:
                \begin{enumerate}
                    \item The user registers in the application. There is a boolean field, \textbf{ConfirmedEmail}, that is marked as false and a string field, \textbf{TokenEmailConfirmation}, that is initialized with a new value.
                    \item The user is asked to check their email box.
                    \item The user clicks a link in the email sent to them. This link sends the user to a page that will validate the \textit{TokenEmailConfirmation} sent in the \textit{Param String} of the page location sending a request to the API.
                    \item The user's email is now validated, so the field \textit{ConfirmedEmail} is true. 
                \end{enumerate}

            \paragraph{Login flux}
                In order to log in the application, the email and password are checked, as well as the \textbf{ConfirmedEmail}. It should be true.

            \paragraph{Password recovery flux}
                To recover a password, I have to ensure that the user requesting the new password is the actual user. In order to do that, I have another field, \textbf{TokenPasswordRecovery}. This will be \textit{null} until the user requires to set a new password. 
                The application will send an email to the user with a link with this token in the \textit{Param String}, that will be sent in a form with the new password sent by the user. If the token is the same as the one in the database, I am sure that the user has
                accessed the page using their own email.

        \subsubsection{How to set up the email service}
            To set this service up, I created an interface to send the emails. I'm not going to explain in detail all the implementation, but it would be important to know that I create an \textit{SMTP (Simple Mail Transfer Protocol) client} using the library \textbf{MailKit} \cite{MailKit}.
            I authenticate in my own Google account and I send the messages to the users (see code \ref{lst:impl_email}). 

            \lstinputlisting[language=CSharp, captionpos=t,
                    caption={EmailService interface and links for every use case}, 
                    label={lst:impl_email}]
            {code/EmailService.cs}

    \subsection{Pagination and population}
        \subsubsection{Why implementing pagination?}
            Pagination allows to send partial information to the user, just the information the user needs. This way, I avoid to load 
            unneccessary information, reducing CPU time in the backend and making the application looks faster, as the data involved in 
            the communication between the server and the client is less. 

            In the next figure \ref{fig:impl_pagination} I show an example of a paginated response where I get the tests. 

            \begin{figure}[H]
                \centering
                    \includegraphics[width=0.9\textwidth]{assets/pagination.png}
                \caption{Paginated response}
                \label{fig:impl_pagination}
            \end{figure}

            In the header response, there is a field called \textbf{x-pagination}. This has
            more information about the pagination (see response \ref{lst:impl_pagination_json}).

            \lstinputlisting[language=json, captionpos=t,
                caption={Metadata information about the pagination}, 
                label={lst:impl_pagination_json}]
            {code/pagination.json}
        
        \subsubsection{What is population?}
            Following the last example, imagine that I send all the information to the user about every test. 
            The frontend is only displaying general information about the tests, but I'm also sending all the videos to display every test. 
            The network data consumption by the user and the processing cost by the application are higher and the application will be slower.
            Therefore, it is really important to send only the neccessary data via the network to the user.
            
            To implement this, I have \textit{DTO} objects that are general. This way, I can send the general information about the test, which is the exact data the user needs. If needed more, I can "populate" the tests 
            sending all the information related to the questions (see the difference between a paginated and a non-paginated response in figure \ref{fig:impl_populated}).

            \begin{figure}[H]
                \centering
                \begin{subfigure}[T]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/populated_false.png}
                    \caption{Populated=false}
                    \label{fig:impl_populated_false}
                \end{subfigure}
                \hfill
                \begin{subfigure}[T]{0.49\textwidth}
                    \centering
                    \includegraphics[width=\textwidth]{assets/populated_true.png}
                    \caption{Populated=true}
                    \label{fig:impl_populated_true}
                \end{subfigure}
                \caption{Difference between a populated and a non-populated request}
                \label{fig:impl_populated}
            \end{figure}

    \subsection{Health check of the API}
        As Microsoft indicates \cite{Health}, it is good to add checkers for the state of the application. 
        This way, orchestrators, logs and people can check the state of the application and its dependencies.
        I added a check for the application and the database context. 
        In the next figure \ref{fig:impl_health}, you can see the difference of response got from this endpoint.
        
        \begin{figure}[H]
            \centering
            \begin{subfigure}[T]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{assets/healthy.png}
                \caption{Healthy state}
                \label{fig:impl_healthy}
            \end{subfigure}
            \hfill
            \begin{subfigure}[T]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{assets/unhealthy.png}
                \caption{Unhealthy state}
                \label{fig:impl_unhealthy}
            \end{subfigure}
            \caption{API health checks}
            \label{fig:impl_health}
        \end{figure}

        Implement this in \textit{ASP.NET} is very easy. Just this method will do the magic. \\
        \lstinputlisting[language=CSharp, captionpos=t,
                caption={Configuring a health endpoint}, 
                label={lst:impl_health_config}]
        {code/Health.cs}

    \subsection{Documenting the API with code using Swagger}
        Swagger \cite{Swagger} is a tool to document, test and design APIs. When documenting the API with \textit{Swagger}, it creates a JSON file following the \textbf{OpenApi} specification, which allows
        to define an API in a language-agnostic way. Therefore, I can share this document with other apps, i.e \textit{Postman} to avoid rewriting all the endpoints.

        As it is visible in the next figure \ref{fig:impl_swagger_endpoint}, I can document every endpoint in the API using comments. This way, my code and documentation goes together and it helps me to understand the better the functions.
        \begin{figure}[H]
            \centering
                \includegraphics[width=0.9\textwidth]{assets/swagger_comment.png}
            \caption{How to document an endpoint using swagger}
            \label{fig:impl_swagger_endpoint}
        \end{figure}

        The output of documenting with Swagger the API would be this page (see figure \ref{fig:impl_swagger}). I can see all the endpoints of the application, execute them and see the schema definitions.. 
        \begin{figure}[H]
            \centering
                \includegraphics[width=0.9\textwidth]{assets/swagger.png}
            \caption{Swagger main page}
            \label{fig:impl_swagger}
        \end{figure}

    \subsection{Middleware diagram}
        In the next page, I included a diagram (see figure \ref{fig:implementation_middleware}) that represents the flux that a request follows from the first moment it reaches the server until the servers replies to this request.

        The diagram contains all the middleware layers. The ones in yellow are the ones created/configured by me. The rest are simply configured by the framework.
        \begin{itemize}[noitemsep]
            \item The uppest middleware layer is the \textit{Exception handler}. It is in charge of catching the exceptions and processing them.
            \item The \textit{Static files middleware} is in charge of giving access to the filesystem.
            \item The \textit{CORS middleware} is in charge of configuring the \textit{CORS} policy.
            \item The \textit{JWT middleware} is in charge of authorization and authentication. It checks the {JWT token} sent by the user in the request. 
            \item The \textit{Validation middleware} is in charge of validating the objects or fields sent in the {HTTP request}. This way, data sent incorrectly won't be processed. For instance, if the user asked for the tests done by him from tomorrow, this middleware would stop the flux as there can not be tests done from tomorrow. This query wouldn't go to the database. 
        \end{itemize}
            \newpage
            \begin{figure}[H]
                \centering
                    \includegraphics[angle=90, width=\textwidth, height=\textheight]{assets/diagrams/middleware.png}
                \caption{Middleware flux}
                \label{fig:implementation_middleware}
            \end{figure}

    \subsection{Sequence diagrams}
        As all requests should follow the same flux and the use cases are very similar, I created just three sequence diagrams that allowed
        to better understand how to implement each endpoint and to separate the common processing to create middlewares and filters.

        \subsubsection{Sequence diagram, common flux}
            All requests have to go through the middleware flux. There are some middlewares that are common for all requests. These are the \textit{ErrorHandlingMiddleware}, \textit{ValidationMiddleware} and the \textit{AuthenticationHandler}.
            Once the request passes through these middlewares, it reaches the controller. \textit{See figure \ref{fig:implementation_common}}.
                \begin{figure}[H]
                    \centering
                        \includegraphics[width=\textwidth]{assets/diagrams/sequence_common.png}
                    \caption{Common behaviour sequence diagrams}
                    \label{fig:implementation_common}
                \end{figure}

        \subsubsection{Sequence diagram, use case: \textit{Delete a test}}
            The user sends a request to the API to delete a test. The API would respond \textit{Ok} if the test has been succesfully deleted or \textit{Conflict} if an error has ocurred.
            When the request arrives at the controller, the controller delegates in the \textit{TestService} the deletion of the test. It retrieves the test, checks if the test is from the user and deletes the test from the database and from the storage.
            \textit{See figure \ref{fig:implementation_deletetest}}.
                \begin{figure}[H]
                    \centering
                        \includegraphics[width=\textwidth]{assets/diagrams/deletetest.png}
                    \caption{Sequence diagram: delete a test use case}
                    \label{fig:implementation_deletetest}
                \end{figure}

        \subsubsection{Extended version of a sequence diagram, use case: \textit{Get a test}}
            This would be the extended version of the sequence diagram (see figure \ref{fig:implementation_getatest}), including the common part. 
            Once the request passes all the middleware, it reaches the \textit{TestController}. 
            It delegates the action into the \textit{TestService}. This gets the test from the repository, 
            checks if the user is the owner of the test and if everything is okay it returns the test to the controller to be sent to the user.
            If an alternative flux happens, a \textit{BusinessException} will be thrown and the user will receive an error message and code.
                \newpage
                \begin{figure}[H]
                    \centering
                        \includegraphics[angle=90, width=\textwidth, height=\textheight]{assets/diagrams/getatest.png}
                    \caption{Sequence diagram: get a test use case}
                    \label{fig:implementation_getatest}
                \end{figure}

\newpage
\section{Frontend code documentation}
    \subsection{Organization of the project}
        \begin{itemize}[noitemsep]
            \item API: in this directory I defined all calls to the API.
            \item Components: all components that compose the screens.
            \item Helpers: it contains auxiliary functions and definitions of the schemas for the forms.
            \item Models: it contains all object definitions. The main objects are the \textit{DTO}, which are the contracts between the API and the frontend project.
            \item Redux: it defines the \textit{store}, the \textit{slices} and the \textit{thunks}. 
            \item Routers: it defines the router logic. With this logic I also implement \textit{private} and \textit{public} routes.
            \item Screens: it contains all the screens of the application.
            \item Services: here all services for the application are defined. I just developed a \textit{persistence} service that uses the localstorage, but defining the service I'm decoupled from the implementation so I would be able to change it in the future without changing all code in the app.
        \end{itemize}

    \subsection{State management}
        I started using Redux \cite{Redux} (see figure \ref{fig:Redux pattern}) for the state management of the application. This was the only solution that I knew and it pretty much helped me a lot, also for testing purposes. 
        This library is based on a pattern that receives the same name. 
        There is a global shared state, that can be consulted by the components in the application and can only be updated through 
        events or actions. These actions update the application's state based on the new arguments and the last state. 
        \begin{figure}[H]
            \centering
                \includegraphics[width=0.6\textwidth]{assets/diagrams/redux.png}
            \caption{Redux pattern}
            \label{fig:Redux pattern}
        \end{figure}

        Although there is a new version of the Redux library, which is what I use, and it has been proved in production environments, 
        there are some critics to this pattern \cite{NoRedux}, due to that a global state could not be neccessary. 
        As I was aware of that I separated use only a global state when it is really needed, as 
        there are other ways to share the state between components, i.e using \textit{props} or other \textit{hooks} such as \textit{useReducer}. 

        Coming again to Redux, it provides a good set of tools that helps the developer to test and debug the application. The next figure \ref{fig:Redux devtools} shows a tool that helps to follow the application state, update the state dispatching actions and
        watching the actions dispatched by the application itself.
        \begin{figure}[H]
            \centering
                \includegraphics[width=0.7\textwidth]{assets/redux_devtools.png}
            \caption{Redux devtools}
            \label{fig:Redux devtools}
        \end{figure}

        Also, Redux allows to manage the API calls in an easier way. I can update the store's state depending on a success, error on pending state as
        shown on the \textit{Register} case in the next code fragment \ref{lst:impl_redux_api}.
        \lstinputlisting[language=javascript, captionpos=t,
                caption={Updating the store's state depending on the API result}, 
                label={lst:impl_redux_api}]
        {code/redux_api.ts}

        There is a restriction, which is that the action that is dispatched to update the store should be syncronous. With the library \textit{thunk}, I can send async actions that will update the state when resolved. \textit{See code fragment \ref{lst:impl_redux_thunk}.}
        \lstinputlisting[language=javascript, captionpos=t,
                caption={Redux thunk}, 
                label={lst:impl_redux_thunk}]
        {code/redux_thunk.ts}

    \subsection{Private VS public routes}
        As the application requires authorization and authentication to work, I need to protect the application routes so that the user always logs in the application. \\

        I implement this functionality with two custom higher-components: \textit{PrivateRoute} and \textit{PublicRoute}. All these components render whatever is inside them (\textit{children of type any}). \\
        The logic is in the \textit{return} statement:
        \begin{itemize}[noitemsep]
            \item The \textit{PrivateRoute} returns the inner-component as soon as it finds a valid \textit{jwt token}. If not, it redirects to an \textit{auth} route.
            \item The \textit{PublicRoute} returns the inner-component as soon as it does not find a valid \textit{jwt token}. This is because if the user has already been authenticated, it will be redirected to an app route. This way I avoid to enter an auth route being already authenticated.
        \end{itemize}
        \lstinputlisting[language=javascript, captionpos=t,
            caption={Protecting application routes}, 
            label={lst:impl_routes}]
        {code/routes.ts}

        The router logic is delegated to a third-party library. I use two subrouters, one for \textit{authorization} (public) routes, and another for \textit{app/dashboard} (private) routes. \\
        \lstinputlisting[language=javascript, captionpos=t,
            caption={Use of public/private routes in router}, 
            label={lst:impl_routers}]
        {code/router.ts}

    \subsection{Intelligent forms using Formik}
        The forms have a huge amount of logic behind that is reusable. To avoid recreating all this code and start focusing on the app as soon as possible, I used a third party library called \textbf{Formik} \cite{Formik}.
        In order to use a form this way, I need to create a validation schema and set the initial values of the forms. Usually, all forms are empty by default and the \textit{submit button} is disabled until the
        form's state matches the validation schema.
        As recommended by \textit{Formik}, I use a third party library to create these object schemas called \textit{Yup} \cite{Yup}.

        \begin{figure}[H]
            \centering
            \begin{subfigure}[T]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{assets/form_incorrect.png}
                \caption{Incorrect state}
                \label{fig:impl_form_incorrect}
            \end{subfigure}
            \hfill
            \begin{subfigure}[T]{0.49\textwidth}
                \centering
                \includegraphics[width=\textwidth]{assets/form_correct.png}
                \caption{Correct state}
                \label{fig:impl_form_correct}
            \end{subfigure}
            \caption{Controlling the form state}
            \label{fig:impl_form}
        \end{figure}

    \subsection{Persisting the data}
        To improve the user experience, data should be persisted to avoid logging out the user from the application each time the page is refreshed.
        This way, I can simulate an active session in the browser. 
        As the way to persistence this data may change, I encapsulated this logic in an object that could be changed in the future. \\

        As I want this data to be stored in the browser rather than in the server, I would have to use the \textit{SessionStorage} or the \textit{LocalStorage}. 
        The main difference between these two is that the data in the \textit{LocalStorage} is avalaible for every tab in the active instance of the browser. On the 
        other hand, the data saved in the \textit{SessionStorage} is destroyed each time the user closes the tab. \\

        In the next code fragment, I show how to maintain the session up if the user refreshed the browser's tab. 
        The current user is saved in the \textit{SessionStorage} when the user logs in the application. 
        This way, the data is already there. If the user refreshes the page, the current state of the application would be constructed 
        from the \textit{SessionStorage}'s data. If there is no user saved there, the user has not an active session, so an invalid user is set.
        \lstinputlisting[language=javascript, captionpos=t,
            caption={How keeping the session active}, 
            label={lst:impl_persisting}]
        {code/initialState.ts}

\newpage
\section{AI service code documentation}
    \subsection{Protecting the API using JWT}
        Decorators \cite{Decorator} are a powerful tool to change the behaviour of a function or class.
        They allow to extend the behavior of the inner function or class without changing the implementation.

        For example, without changing the implementation of the controller, I can add a function to protect it with a JWT.
        The decorator \ref{lst:impl_decorator} checks the HTTP header, particularly the \textbf{x-access-token} field. It gets the JWT from there and
        tries to decode it using the same \textit{Secret key} used in the \textit{Backend} project to code it. This way, only
        users with a valid JWT from the other project can use this API.
        \lstinputlisting[language=Python, captionpos=t, breaklines=true,
            caption={Decorator to check a JWT}, 
            label={lst:impl_decorator}]
        {code/decorator.py}

    \subsection{Validating the videos}
        The video sent to the API should have a valid extension and its filename should not be empty.
        To validate the filename I use this auxiliary function \ref{lst:impl_validate_video}. 
        \lstinputlisting[language=Python, captionpos=t, breaklines=true,
            caption={Validating a video sent to the API}, 
            label={lst:impl_validate_video}]
        {code/validate_video.py}

    \subsection{Initializing the model}
        As every model performs the better with a different dataset, I initialize a model for each dataset, so I have three active models. 
        In terms of performance, it is recommended to initialize the model outside the controller and have just an instance per thread as 
        shown in the next fragment of code \ref{lst:impl_setup_model}.

        \lstinputlisting[language=Python, captionpos=t, breaklines=true,
            caption={Setting up a model}, 
            label={lst:impl_setup_model}]
        {code/setup_model.py}

        As the Flask documentation indicates, for production purposes it is recommended to use streamer services \cite{ServiceStreamer} to let
        the users send multiple videos/images and maximize the paralleling processing capacity of the GPU.

    \subsection{Executing the model}
        \subsubsection{Video transformations}
            The transformations applied are the indicated in the paper \cite{Li2019}. 
            \begin{itemize}[noitemsep]
                \item Load RGB frames from the video 
                \item Horizontally flip the video with a probability of 0.5
                \item Crop the video (centering, 224px)
            \end{itemize}
            
            The next fragment of code \ref{lst:impl_transformations} shows how to implement all those transformations.
            \lstinputlisting[language=Python, captionpos=t, breaklines=true,
                caption={Video transformations}, 
                label={lst:impl_transformations}]
            {code/transformations.py}

            The model prediction is a classification of the best labels that match the video. To return a correct value to the API, I send the
            best 10 words that match the video. In order to do that, I just need to get the word that matches the label, which comes from a dictionary.
   
\newpage
\section{Communication between the Backend and the AI Service}
    The \textit{Backend} and the \textit{AI Service} are two completely independent modules. To allow them to communicate, the \textit{AI Service} includes a \textbf{REST API} 
    that exposes a function to be called from other services, in this case the \textit{Backend}. 

    As shown in the next figure, the user communicates with the backend of the application using the frontend in their device.
    The use case in which the backend communicates with the \textit{AI Service} is the one in which the user replies to a question. The user sends the reply to the question by clicking on a button in their device, and a form is sent to 
    the server. The backend of the application gets the request and tries to update the database with the user reply. In case the test is a \textit{Mimic or QA test}, the backend is not 
    able by itself to validate if the user's reply is correct or not, as the user's reply is a video signing the given word. 
    \textit{See figure \ref{fig:impl_comm}}.

    \begin{figure}[H]
        \centering
            \includegraphics[width=\textwidth]{assets/diagrams/comm.png}
        \caption{Communication between services}
        \label{fig:impl_comm}
    \end{figure}

    In case the test is of type \textit{Mimic or QAs}, the backend will start communication with an extern service, the \textit{AI Service} sending the user's video to classify it. The communication is made via an API, 
    so the backend will send an HTTP request to this endpoint. The response from this endpoint will be a list of words classified by the service. If the word to sign is amongst these words, the backend will 
    update the question in the database as a correct question. If the classification does not include the given word, the backend will update the question in the database as a failed question.

    I am going to show the neccessary code \ref{lst:impl_reply} to implement the communication. \textit{I'm skipping some code to make it easier to read.}
    The controller receives the request and delegates the functionality to the correct service. In this case, the \textit{QuestionService}. 
    The main responsability of the \textit{UpdateQuestion} in the \textit{QuestionService} is to update the question in the database. It will get the 
    repository and the question with all the updated parameters and then delegates the responsability of updation to the correct repository. 
    \lstinputlisting[language=CSharp, captionpos=t, breaklines=true,
            caption={Controller delegating the service the updation of the question}, 
            label={lst:impl_reply}]
    {code/Reply.cs}

    The function to get the updated parameters \ref{lst:impl_replyi} is in charge of checking if the question is correct or not. The initial parameters are 
    the ones that the question already has, so it starts getting the question from the repository. Then, it checks the type of the test. In case the test is 
    a \textit{Mimic or QA test}, it is needed to communicate with the \textit{AI Service} to check if the question is correct or not. 
    The question will be correct if the extern service includes the asked word to sign amongst the top 10 predicted words from the user's video 
    As the responsability of this function is not to do the communication but to communicate when neccessary, 
    it will delegates this to the \textit{AIService}, which is an abstraction of the real service, which is extern to this application. 
    \lstinputlisting[language=CSharp, captionpos=t, breaklines=true,
            caption={The service gets the parameters to update the question in the Database. For some tests, it communicates with the \textit{AI Service} to get them.}, 
            label={lst:impl_replyi}]
    {code/ReplyI.cs}

    As the \textit{AI Service} is an extern independent module that initializes a Deep Learning model and executes it, it is needed an API to 
    communicate with others services. This service includes an HTTP API, so to call it I just need to send an HTTP request. 
    
    The request should have some parameters to execute the model correctly, such as the difficulty to select the model and the video itself to classify it. 
    Also, it should includes the authorization to avoid the restrict the access to this resource. 

    The reply will come as a string, so it should be mapped into an object to work with this classification and try to find the asked word. It will be mapped to a list 
    of 10 elements, the top ten classifications made by the deep learning model. The next code fragment \ref{lst:impl_replyii} shows how the backend communicates 
    with the \textit{AI Service}.
    \lstinputlisting[language=CSharp, captionpos=t, breaklines=true,
            caption={How the backend communicates with the extern service sending an HTTP request}, 
            label={lst:impl_replyii}]
    {code/ReplyII.cs}


