@misc{gplv3,
    author       = {Free Software Foundation},
    title        = {{GNU} {G}eneral {P}ublic {L}icense},
    version      = {3},
	howpublished = {\url{http://www.gnu.org/licenses/gpl.html}},
    shorthand    = {GPL},
}

@misc{GMeet,
   author = {Google},
   title  = {Google Meet},
   howpublished = {\url{https://meet.google.com}}
}

@misc{Waterfall,
   author = {Wikimedia Commons},
   title  = {Waterfall model},
   howpublished = {\url{https://commons.wikimedia.org/wiki/File:Waterfall\_model.svg}}
}

@misc{Scrum,
   author = {Wikimedia Commons},
   title  = {Scrum model},
   howpublished = {\url{https://commons.wikimedia.org/wiki/File:Scrum\_process.svg}}
}

@misc{Figma,
   author = {Figma},
   title  = {Figma},
   howpublished = {\url{https://www.figma.com}}
}

@misc{Notion,
   author = {Notion},
   title = {Notion},
   howpublished = {\url{https://notion.so}}
}

@article{Li2019,
   abstract = {Vision-based sign language recognition aims at helping deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge, it is by far the largest public ASL dataset to facilitate word-level sign recognition research. Based on this new large-scale dataset, we are able to experiment with several deep learning methods for word-level sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance-based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that models spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 66% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at \url\{https://dxli94.github.io/WLASL/\}.},
   author = {Dongxu Li and Cristian Rodriguez Opazo and Xin Yu and Hongdong Li},
   month = {10},
   title = {Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison},
   url = {http://arxiv.org/abs/1910.11006},
   year = {2019},
}
@article{Sadeddine2021,
   abstract = {Static hand gesture (HG) recognition for both user-dependent and user-independent is a challenging problem, especially when there are changes in lighting, hand position, and background, the recognition becomes more complex. To solve this problem, this paper proposes a static hand gesture recognition based on a set of image descriptors: Gradient Local Auto-Correlation (GLAC), Gabor Wavelet Transform (GWT), and Fast Discrete Curve Transform (FDCT). Principal Component Analysis (PCA) was used to reduce dimensionality. Tests were performed on three sign language datasets and one hand posture dataset using neural network classifiers, K-Nearest Neighbor (KNN) classifiers, and combined classifiers. The results obtained were compared to the state of the art and show an accuracy of 100% for user-independent and 98.33% for user-dependent gestures, despite the difficult acquisition conditions of the datasets.},
   author = {Khadidja Sadeddine and Zohra Fatma Chelali and Rachida Djeradi and Amar Djeradi and Sidahmed Ben Abderrahmane},
   doi = {10.1016/j.jvcir.2021.103193},
   issn = {10959076},
   journal = {Journal of Visual Communication and Image Representation},
   keywords = {Combined classifiers,Curvelet transform,GLAC,Gabor wavelet,Sign language recognition,Static hand gesture recognition},
   month = {8},
   publisher = {Academic Press Inc.},
   title = {Recognition of user-dependent and independent static hand gestures: Application to sign language},
   volume = {79},
   year = {2021},
}
@article{Fregoso2021,
   abstract = {This paper presents an approach to design convolutional neural network architectures, using the particle swarm optimization algorithm. The adjustment of the hyper-parameters and finding the optimal network architecture of convolutional neural networks represents an important challenge. Network performance and achieving efficient learning models for a particular problem depends on setting hyper-parameter values and this implies exploring a huge and complex search space. The use of heuristic-based searches supports these types of problems; therefore, the main contribution of this research work is to apply the PSO algorithm to find the optimal parameters of the convolutional neural networks which include the number of convolutional layers, the filter size used in the convolutional process, the number of convolutional filters, and the batch size. This work describes two optimization approaches; the first, the parameters obtained by PSO are kept under the same conditions in each convolutional layer, and the objective function evaluated by PSO is given by the classification rate; in the second, the PSO generates different parameters per layer, and the objective function is composed of the recognition rate in conjunction with the Akaike information criterion, the latter helps to find the best network performance but with the minimum parameters. The optimized architectures are implemented in three study cases of sign language databases, in which are included the Mexican Sign Language alphabet, the American Sign Language MNIST, and the American Sign Language alphabet. According to the results, the proposed methodologies achieved favorable results with a recognition rate higher than 99%, showing com-petitive results compared to other state-of-the-art approaches.},
   author = {Jonathan Fregoso and Claudia I. Gonzalez and Gabriela E. Martinez},
   doi = {10.3390/axioms10030139},
   issn = {20751680},
   issue = {3},
   journal = {Axioms},
   keywords = {Optimization of convolutional neural networks,PSO,Sign language recognition},
   month = {9},
   publisher = {MDPI AG},
   title = {Optimization of convolutional neural networks architectures using pso for sign language recognition},
   volume = {10},
   year = {2021},
}
@report{SLTTSPNet,
   abstract = {Sign language translation (SLT) aims to interpret sign video sequences into text-based natural language sentences. Sign videos consist of continuous sequences of sign gestures with no clear boundaries in between. Existing SLT models usually represent sign visual features in a frame-wise manner so as to avoid needing to explicitly segmenting the videos into isolated signs. However, these methods neglect the temporal information of signs and lead to substantial ambiguity in translation. In this paper, we explore the temporal semantic structures of sign videos to learn more discriminative features. To this end, we first present a novel sign video segment representation which takes into account multiple temporal granularities, thus alleviating the need for accurate video segmentation. Taking advantage of the proposed segment representation, we develop a novel hierarchical sign video feature learning method via a temporal semantic pyramid network, called TSPNet. Specifically, TSPNet introduces an inter-scale attention to evaluate and enhance local semantic consistency of sign segments and an intra-scale attention to resolve semantic ambiguity by using non-local video context. Experiments show that our TSPNet outperforms the state-of-the-art with significant improvements on the BLEU score (from 9.58 to 13.41) and ROUGE score (from 31.80 to 34.96) on the largest commonly-used SLT dataset. Our implementation is available at https://github.com/verashira/TSPNet.},
   author = {Dongxu Li and Chenchen Xu and Xin Yu and Kaihao Zhang and Ben Swift and Hanna Suominen and Hongdong Li},
   title = {ANU) 1 , Australian Centre for Robotic Vision (ACRV) 2 , Data61 / CSIRO 3 , University of Technology Sydney (UTS) 4 , Tencent AI Lab 5},
   url = {https://github.com/verashira/TSPNet},
}
@article{Min2021,
   abstract = {Vision-based Continuous Sign Language Recognition (CSLR) aims to recognize unsegmented gestures from image sequences. To better train CSLR models, the iterative training scheme is widely adopted to alleviate the overfitting of the alignment model. Although the iterative training scheme can improve performance, it will also increase the training time. In this work, we revisit the overfitting problem in recent CTC-based CSLR works and attribute it to the insufficient training of the feature extractor. To solve this problem, we propose a Visual Alignment Constraint (VAC) to enhance the feature extractor with more alignment supervision. Specifically, the proposed VAC is composed of two auxiliary losses: one makes predictions based on visual features only, and the other aligns short-term visual and long-term contextual features. Moreover, we further propose two metrics to evaluate the contributions of the feature extractor and the alignment model, which provide evidence for the overfitting problem. The proposed VAC achieves competitive performance on two challenging CSLR datasets and experimental results show its effectiveness.},
   author = {Yuecong Min and Aiming Hao and Xiujuan Chai and Xilin Chen},
   month = {4},
   title = {Visual Alignment Constraint for Continuous Sign Language Recognition},
   url = {http://arxiv.org/abs/2104.02330},
   year = {2021},
}
@article{ELM2021,
   abstract = {Hearing impaired individuals can easily overcome the barriers in communicating with other members of the society via computer technology. In this study, the recognition of dynamic words in Turkish Sign Language (TSL) with two hands was studied using the Leap Motion Controller (LMC) device. 50 dynamic words were determined considering the similarities and differences among themselves, and a dataset was created using 4 signers. For the system proposed in this paper, a comprehensive feature extraction process is executed. Applying feature selection algorithms and PCA, LDA and PCA+LDA dimension reduction methods to this dataset, new datasets with less dimension were obtained. For the first time, ELM architectures were operated as a classifier in a sign language recognition system. Recognition performance was tested with 5 different ELM networks and 2 classical classifiers and the results were compared. In addition, comparisons of classical and ELM based classifiers were presented. The 10-fold cross validation method was used to test the validity of the proposed system and the accuracy of the results obtained. Based on the results obtained by a comprehensive analysis, it was observed that the ML-KELM classifier maintains its performance rate and gives the highest performance rate. At the same time, it has been observed that ML-KELM classifier has a stable structure, which offers less user intervention.},
   author = {Zekeriya Katılmış and Cihan Karakuzu},
   doi = {10.1016/j.eswa.2021.115213},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   keywords = {Dynamic word gestures,Extreme Learning Machine (ELM),Hand gesture recognition,Leap Motion Controller (LMC),Sign language recognition},
   month = {11},
   publisher = {Elsevier Ltd},
   title = {ELM based two-handed dynamic Turkish Sign Language (TSL) word recognition},
   volume = {182},
   year = {2021},
}
@article{Venugopalan2021,
   abstract = {One of the major challenges that deaf people face in modern societal life is communication. For those engaged in agricultural jobs, efficiency at work and productivity are deeply related to the quality of deciphering the sign language used by the deaf farmers. Employing sign language interpreters is not a pragmatic solution to this problem. There comes the need for developing a reliable system for automatic sign language recognition (SLR). This paper reports a work on the recognition of hand gestures for the Indian sign language (ISL) words commonly used by deaf farmers. A hybrid deep learning model with convolutional long short term memory (LSTM) network has been exploited for gesture classification. The model has attained an average classification accuracy of 76.21% on the proposed dataset of ISL words from the agricultural domain.},
   author = {Adithya Venugopalan and Rajesh Reghunadhan},
   doi = {10.1016/j.eswa.2021.115601},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   keywords = {Agricultural words,Complex backgrounds,Deep learning,Dynamic hand gestures,Indian sign language,Sign language recognition},
   month = {12},
   publisher = {Elsevier Ltd},
   title = {Applying deep neural networks for the automatic recognition of sign language words: A communication aid to deaf agriculturists},
   volume = {185},
   year = {2021},
}
@article{Moryossef2020,
   abstract = {We propose a lightweight real-time sign language detection model, as we identify the need for such a case in videoconferencing. We extract optical flow features based on human pose estimation and, using a linear classifier, show these features are meaningful with an accuracy of 80%, evaluated on the DGS Corpus. Using a recurrent model directly on the input, we see improvements of up to 91% accuracy, while still working under 4ms. We describe a demo application to sign language detection in the browser in order to demonstrate its usage possibility in videoconferencing applications.},
   author = {Amit Moryossef and Ioannis Tsochantaridis and Roee Aharoni and Sarah Ebling and Srini Narayanan},
   month = {8},
   title = {Real-Time Sign Language Detection using Human Pose Estimation},
   url = {http://arxiv.org/abs/2008.04637},
   year = {2020},
}
@report{SignLanguageTransformers,
   abstract = {Prior work on Sign Language Translation has shown that having a mid-level sign gloss representation (effectively recognizing the individual signs) improves the translation performance drastically. In fact, the current state-of-the-art in translation requires gloss level tokenization in order to work. We introduce a novel transformer based architecture that jointly learns Continuous Sign Language Recognition and Translation while being trainable in an end-to-end manner. This is achieved by using a Connectionist Temporal Classification (CTC) loss to bind the recognition and translation problems into a single unified architecture. This joint approach does not require any ground-truth timing information , simultaneously solving two co-dependant sequence-to-sequence learning problems and leads to significant performance gains. We evaluate the recognition and translation performances of our approaches on the challenging RWTH-PHOENIX-Weather-2014T (PHOENIX14T) dataset. We report state-of-the-art sign language recognition and translation results achieved by our Sign Language Transformers. Our translation networks outperform both sign video to spoken language and gloss to spoken language translation models, in some cases more than doubling the performance (9.58 vs. 21.80 BLEU-4 Score). We also share new baseline translation results using transformer networks for several other text-to-text sign language translation tasks.},
   author = {Necati Cihan Camgöz and Oscar Koller and Simon Hadfield and Richard Bowden},
   title = {Sign Language Transformers: Joint End-to-end Sign Language Recognition and Translation},
}
@generic{Bronstein2015,
   author = {Michael M. Bronstein and Lourdes Agapito and Carsten Rother},
   doi = {10.1007/978-3-319-16178-5},
   isbn = {9783319161983},
   issn = {16113349},
   journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
   pages = {VI},
   publisher = {Springer Verlag},
   title = {Sign Language Recognition Using ConvolutionalNeural Networks},
   volume = {8927},
   year = {2015},
}
@misc{TikTok2021,
   author = {Martin Cristobal Balasch},
   title = {La INFRAESTRUCTURA detrás de TikTok},
   howpublished = {\url{https://www.youtube.com/watch?v=8OcPdAbSKMU}}
}
@misc{RT2021,
   author = {Moryosse Amit and Tsochantaridis Ioannis and Aharoni Roee and Ebling Sarah and Narayanan Srini},
   title = {YouTube - Real- Time Sign Language Detection for Video Conferencing Applications.},
   howpublished = {\url{https://www.youtube.com/watch?v=11\_GWjoCtd4\&list=PLZ\_UieHZjKhWDFh2XknFFULOtFV-eUOTI\&index=9},}
}
@misc{ASLRecognitionPython,
   author = {Makwana Rahul},
   title = {American Sign Language Recognition in Python using Deep Learning},
   howpublished = {\url{https://coderspacket.com/american-sign-language-recognition-in-python-using-deep-learning},}
}
@generic{Vagdevi2019,
   author = {Vagdevi},
   month = {3},
   title = {How to use transfer learning for sign language recognition},
   url = {https://www.freecodecamp.org/news/asl-recognition-using-transfer-learning-918ba054c004/},
   year = {2019},
}
@generic{Vagdevi2019i,
   author = {Vagdevi},
   month = {1},
   title = {How to build a convolutional neural network that recognizes sign language gestures},
   url = {https://www.freecodecamp.org/news/asl-using-alexnet-training-from-scratch-cfec9a8acf84/},
   year = {2019},
}
@article{Vaishshells2021,
   author = {Vaishshells},
   journal = {Data Science Blogathon},
   month = {6},
   title = {Sign Language Recognition for Computer Vision Enthusiasts},
   url = {https://www.analyticsvidhya.com/blog/2021/06/sign-language-recognition-for-computer-vision-enthusiasts/},
   year = {2021},
}
@article{Akash2018,
   author = {Akash},
   title = {ASL Alphabet},
   url = {https://www.kaggle.com/grassknoted/asl-alphabet},
   year = {2018},
}
@generic{ContinuousSignLanguageRecognition2021,
   author = {Min Yuecong},
   month = {8},
   title = {Visual Alignment Constraint for Continuous Sign Language Recognition},
   url = {https://github.com/ycmin95/VAC\_CSLR},
   year = {2021},
}
@generic{SignLanguageClassification2020,
   author = {Kumar Vaibhav},
   month = {6},
   title = {Hands-On Guide To Sign Language Classification Using CNN},
   url = {https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/https://analyticsindiamag.com/hands-on-guide-to-sign-language-classification-using-cnn/},
   year = {2020},
}
@generic{Rastgoo2021,
   abstract = {Sign language, as a different form of the communication language, is important to large groups of people in society. There are different signs in each sign language with variability in hand shape, motion profile, and position of the hand, face, and body parts contributing to each sign. So, visual sign language recognition is a complex research area in computer vision. Many models have been proposed by different researchers with significant improvement by deep learning approaches in recent years. In this survey, we review the vision-based proposed models of sign language recognition using deep learning approaches from the last five years. While the overall trend of the proposed models indicates a significant improvement in recognition accuracy in sign language recognition, there are some challenges yet that need to be solved. We present a taxonomy to categorize the proposed models for isolated and continuous sign language recognition, discussing applications, datasets, hybrid models, complexity, and future lines of research in the field.},
   author = {Razieh Rastgoo and Kourosh Kiani and Sergio Escalera},
   doi = {10.1016/j.eswa.2020.113794},
   issn = {09574174},
   journal = {Expert Systems with Applications},
   keywords = {Application,Computer Vision,Deep learning,Face recognition,Pose estimation,Sign language recognition},
   month = {2},
   publisher = {Elsevier Ltd},
   title = {Sign Language Recognition: A Deep Survey},
   volume = {164},
   year = {2021},
}
@misc{SignLanguageRecognitionUsingActionRecognition,
   author = {Renotte Nicholas},
   title = {YouTube - Sign Language Detection using ACTION RECOGNITION with Python | LSTM Deep Learning Model},
   howpublished = {\url{https://www.youtube.com/watch?v=doDUihpj6ro\&list=PLZ\_UieHZjKhWDFh2XknFFULOtFV-eUOTI\&index=4\&t=241s},}
}
@misc{SignLanguageRecognitionUsingObjectDetection,
   author = {Renotte Nicholas},
   title = {YouTube - Real Time Sign Language Detection with Tensorflow Object Detection and Python | Deep Learning SSD},
   howpublished = {\url{https://www.youtube.com/watch?v=pDXdlXlaCco\&list=PLZ\_UieHZjKhWDFh2XknFFULOtFV-eUOTI\&index=1\&t=353s},}
}
@generic{SignLanguageRecognitionUsingReactjsAndTensorflow,
   abstract = {
Maybe, you want to do exactly this and help solve the sign language barrier!

Well you can!

In this video you’ll learn how to do exactly that in 45 minutes. You’ll convert an existing Python Tensorflow model and leverage it in a standalone app with real time detections. You’ll be able to see the results in real time! But what’s even better is that you can take this and apply to to your very own use cases, whatever they might be!

This video covers: 
1. Converting a Tensorflow Object Detection API model to Tensorflow.JS Graph Model format
2. Hosting a trained Tensorflow deep learning model for applications
3. Downloading the React and Tensorflow.JS  Computer Vision Template
4. Making real time detections using a deployed Tensorflow.JS model
5. Visualising detections within the HTML canvas

Completed Code: 
https://github.com/nicknochnack/RealT...

Other Links
Template Code: https://github.com/nicknochnack/React...
Setting Up Cloud Object Storage on IBM Cloud: https://cloud.ibm.com/objectstorage/c...
Installing the Cloud Object Storage CLI: https://github.com/IBM-Cloud/ibm-clou...
Installing the Cloud Object Storage Plugin: https://github.com/IBM/ibmcloud-objec... 

Tensorflow.JS : https://www.tensorflow.org/js
React: https://reactjs.org/},
   author = {Renotte Nicholas},
   month = {11},
   title = {YouTube - Building a Real Time Sign Language Detection App with React.JS and Tensorflow.JS | Deep Learning},
   url = {https://www.youtube.com/watch?v=ZTSRZt04JkY\&list=PLZ\_UieHZjKhWDFh2XknFFULOtFV-eUOTI\&index=3},
   year = {2020},
}
@article{SpanishDataset2021,
   abstract = {Around 5% of the world population suffers from hearing impairment. One of its main barriers is communication with others since it could lead to their social exclusion and frustration. To overcome this issue, this paper presents a system to interpret the Spanish sign language alphabet which makes the communication possible in those cases, where it is necessary to sign proper nouns such as names, streets, or trademarks. For this, firstly, we have generated an image dataset of the signed 30 letters composing the Spanish alphabet. Then, given that there are static and in-motion letters, two different kinds of neural networks have been tested and compared: convolutional neural networks (CNNs) and recurrent neural networks (RNNs). A comparative analysis of the experimental results highlights the importance of the spatial dimension with respect to the temporal dimension in sign interpretation. So, CNNs obtain a much better accuracy, with 96.42% being the maximum value.},
   author = {Ester Martinez-Martin and Francisco Morillas-Espejo},
   doi = {10.1155/2021/5532580},
   issn = {1687-5265},
   journal = {Computational Intelligence and Neuroscience},
   month = {6},
   pages = {1-10},
   publisher = {Hindawi Limited},
   title = {Deep Learning Techniques for Spanish Sign Language Interpretation},
   volume = {2021},
   year = {2021},
}

@article{Kinetics400,
   author = {J. Carreira and A. Zisserman},
   title = {Quo vadis, action recognition? a new model and the kinetics dataset},
   year = {2017},
   journal = {CVPR},
}

@article{ImageNet,
   author = {O. Russakovsky, J. Deng, H. Su},
   title = {ImageNet Large Scale Visual Recognition Challenge},
   year = {2015},
   journal = {International Journal of Computer Vision (IJCV)},
}

@misc{DI,
   author = {Microsoft},
   title = {Dependency Injection},
   howpublished = {\url{https://docs.microsoft.com/en-us/dotnet/core/extensions/dependency-injection},}
}

@misc{JWT.io,
   author = {auth0},
   title = {JWT},
   howpublished = {\url{https://jwt.io/}}
}

@misc{MailKit,
   author = {Jeffrey Stedfast},
   title = {MimeKit},
   howpublished = {\url{http://www.mimekit.net/}}
}

@misc{CORS,
   author = {MDN},
   title = {Cross-Origin Resource Sharing (CORS)},
   howpublished = {\url{https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS}   }
}

@misc{FluentValidation,
   author = {FluentValidation},
   title = {FluentValidation documentation},
   howpublished = {\url{https://docs.fluentvalidation.net/en/latest/}}
}

@misc{Hash,
   author = {Microsoft},
   title = {Hash passwords in ASP.NET Core},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/core/security/data-protection/consumer-apis/password-hashing?view=aspnetcore-5.0}}
}

@misc{DTO,
   author = {Microsoft},
   title = {Create Data Transfer Objects (DTO)},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/web-api/overview/data/using-web-api-with-entity-framework/part-5}}
}

@misc{AutoMapper,
   author = {AutoMapper},
   title = {AutoMapper},
   howpublished = {\url{https://automapper.org/}}
}

@misc{Kimutis,
   author = {Donatas Kimutis},
   title = {Kimutis},
   howpublished = {\url{http://kimutis.lt/slides}}
}

@misc{RepoAndUW,
   author = {Microsoft},
   title = {Implementing the Repository and Unit of Work Patterns in an ASP.NET MVC Application (9 of 10)},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/mvc/overview/older-versions/getting-started-with-ef-5-using-mvc-4/implementing-the-repository-and-unit-of-work-patterns-in-an-asp-net-mvc-application}}
}

@misc{Filters,
   author = {Microsoft},
   title = {Filters in ASP.NET Core},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/core/mvc/controllers/filters?view=aspnetcore-5.0}}
}

@misc{AKV,
   author = {Microsoft},
   title = {Azure Key Vault configuration provider in ASP.NET Core},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/core/security/key-vault-configuration?view=aspnetcore-5.0},}
}

@misc{UserSecrets,
   author = {Microsoft},
   title = {Safe storage of app secrets in development in ASP.NET Core},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/core/security/app-secrets?view=aspnetcore-5.0\&tabs=windows}}
}

@misc{MVC,
   author = {Wikipedia},
   title = {Model view controller},
   howpublished = {\url{https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93controller}}
}

@misc{OnionArchitecture,
   author = {Lori Peterson},
   title = {Onion Architecture},
   howpublished = {\url{https://lorifpeterson.com/?p=64}}
}

@misc{Health,
   author = {Microsoft},
   title = {Health checks in ASP.NET Core},
   howpublished = {\url{https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/health-checks?view=aspnetcore-6.0}}
}

@misc{Swagger,
   author = {SMARTBEAR},
   title = {Swagger},
   howpublished = {\url{https://swagger.io/}}
}

@misc{NoRedux,
   author = {Matías Hernández},
   title = {No necesitas Redux},
   howpublished = {\url{https://matiashernandez.dev/blog/post/no-necesitas-redux}}
}

@misc{Redux,
   author = {Dan Abramov},
   title = {Redux},
   howpublished = {\url{https://redux.js.org/}}
}

@misc{ReduxDiagram,
   author = {Andra Joy Lally},
   title = {An Obsession with Design Patterns: Redux},
   howpublished = {\url{https://engineering.zalando.com/posts/2016/08/design-patterns-redux.html}}
}

@misc{Formik,
   author = {Formium},
   title = {Formik},
   howpublished = {\url{https://formik.org/}}
}

@misc{Yup,
   author = {jquense},
   title = {Yup},
   howpublished = {\url{https://github.com/jquense/yup}}
}

@misc{Decorator,
   author = {Sheetansh kumar},
   title = {Decorators in Python},
   howpublished = {\url{https://www.geeksforgeeks.org/decorators-in-python/}}
}

@misc{ServiceStreamer,
   author = {ShannonAI},
   title = {Service streamer},
   howpublished = {\url{https://github.com/ShannonAI/service-streamer}}
}

@misc{Coverage,
   author = {Nayan Raval},
   title = {How To Generate Code Coverage Report in HTML Using *.coverage},
   howpublished = {\url{https://www.thecodehubs.com/how-to-generate-code-coverage-report-in-html-using-coverage-2/}}
}

@misc{Chai,
   title = {Chai Assertion Library},
   author = {Jake Luer},
   howpublished = {\url{https://www.chaijs.com/}}
}